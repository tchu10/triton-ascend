# -*- coding: utf-8 -*-
# Copyright (c) Huawei Technologies Co., Ltd. 2024-2025. All rights reserved.
import triton
import triton.language as tl
import torch
import pytest
import test_common
from test_common import TestUtils
import math
def torch_sum(x1,dim):
    if x1.dtype == torch.float16 or x1.dtype == torch.bfloat16:
        res = torch.sum(x1.to(torch.float32), dim, keepdim=False).to(x1.dtype)
    else:
        res = torch.sum(x1, dim, keepdim=False).to(x1.dtype)
    return res

@triton.jit
def tt_sum_1d(in_ptr, out_ptr,
    xnumel:tl.constexpr, ynumel:tl.constexpr, znumel:tl.constexpr,
    XB:tl.constexpr, YB:tl.constexpr, ZB:tl.constexpr, dim:tl.constexpr):
    idx = tl.arange(0,XB)
    x = tl.load(in_ptr + idx)
    ret = tl.sum(x,dim)
    tl.store(out_ptr + tl.arange(0,1), ret)

@triton.jit
def tt_sum_2d(in_ptr, out_ptr,
    xnumel:tl.constexpr, ynumel:tl.constexpr, znumel:tl.constexpr,
    XB:tl.constexpr, YB:tl.constexpr, ZB:tl.constexpr, dim:tl.constexpr):
    xoffs = tl.program_id(0) * XB
    yoffs = tl.program_id(1) * YB
    xidx = tl.arange(0,XB) + xoffs
    yidx = tl.arange(0,YB) + yoffs
    idx = xidx[:,None]*ynumel + yidx[None,:]
    
    x = tl.load(in_ptr + idx)
    ret = tl.sum(x, dim)

    if dim == 0:
        oidx = yidx
    else:
        oidx = xidx
    tl.store(out_ptr + oidx, ret)

@triton.jit
def tt_sum_3d(in_ptr, out_ptr,
    xnumel:tl.constexpr, ynumel:tl.constexpr, znumel:tl.constexpr,
    XB:tl.constexpr, YB:tl.constexpr, ZB:tl.constexpr, dim:tl.constexpr):
    xoffs = tl.program_id(0) * XB
    yoffs = tl.program_id(1) * YB
    zoffs = tl.program_id(2) * ZB

    xidx = tl.arange(0,XB) + xoffs
    yidx = tl.arange(0,YB) + yoffs
    zidx = tl.arange(0,ZB) + zoffs

    idx = xidx[:,None,None]*ynumel*znumel + yidx[None,:,None]*znumel + zidx[None,None,:]
    
    x = tl.load(in_ptr + idx)
    ret = tl.sum(x, dim)

    if dim == 0:
        oidx = yidx[:,None]*znumel + zidx[None,:]
    elif dim == 1:
        oidx = xidx[:,None]*znumel + zidx[None,:]
    else :
        oidx = xidx[:,None]*ynumel + yidx[None,:]

    tl.store(out_ptr + oidx, ret)

@triton.jit
def tt_sum_3d_0_1(in_ptr, out_ptr,
    xnumel:tl.constexpr, ynumel:tl.constexpr, znumel:tl.constexpr,
    XB:tl.constexpr, YB:tl.constexpr, ZB:tl.constexpr, dim:tl.constexpr):

    xidx = tl.arange(0,XB)
    yidx = tl.arange(0,YB)
    zidx = tl.arange(0,ZB)

    idx = xidx[:,None,None]*ynumel*znumel + yidx[None,:,None]*znumel + zidx[None,None,:]
    
    x = tl.load(in_ptr + idx)

    tmp = tl.sum(x, 0)
    ret = tl.sum(tmp, 0)
    oidx = zidx

    tl.store(out_ptr + oidx, ret)

@triton.jit
def tt_sum_3d_0_2(in_ptr, out_ptr,
    xnumel:tl.constexpr, ynumel:tl.constexpr, znumel:tl.constexpr,
    XB:tl.constexpr, YB:tl.constexpr, ZB:tl.constexpr, dim:tl.constexpr):

    xidx = tl.arange(0,XB)
    yidx = tl.arange(0,YB)
    zidx = tl.arange(0,ZB)

    idx = xidx[:,None,None]*ynumel*znumel + yidx[None,:,None]*znumel + zidx[None,None,:]
    
    x = tl.load(in_ptr + idx)
    
    tmp = tl.sum(x, 0)
    ret = tl.sum(tmp, 1)
    oidx = yidx

    tl.store(out_ptr + oidx, ret)

@triton.jit
def tt_sum_3d_1_2(in_ptr, out_ptr,
    xnumel:tl.constexpr, ynumel:tl.constexpr, znumel:tl.constexpr,
    XB:tl.constexpr, YB:tl.constexpr, ZB:tl.constexpr, dim:tl.constexpr):

    xidx = tl.arange(0,XB)
    yidx = tl.arange(0,YB)
    zidx = tl.arange(0,ZB)

    idx = xidx[:,None,None]*ynumel*znumel + yidx[None,:,None]*znumel + zidx[None,None,:]
    
    x = tl.load(in_ptr + idx)

    tmp = tl.sum(x, 1)
    ret = tl.sum(tmp, 1)
    oidx = xidx

    tl.store(out_ptr + oidx, ret)

def is_legal_combine(shape,dims):
    return (len(shape) == 3) or \
           (len(dims) == 1 and dims[0] < len(shape))

dims_map = {
    (0, 1): tt_sum_3d_0_1,
    (1, 2): tt_sum_3d_1_2,
    (0, 2): tt_sum_3d_0_2
}

shape_map = {
    1: {"append_shape": (1,1), "func": tt_sum_1d},
    2: {"append_shape": (1,), "func": tt_sum_2d},
    3: {"append_shape": (), "func": tt_sum_3d}
}

@pytest.mark.parametrize('shape', TestUtils.full_shape)
@pytest.mark.parametrize('dtype', ["int8", "int16", "int32", "int64", "float16", "float32", "bfloat16"]) # no bool
@pytest.mark.parametrize('dims', [(0,), (1,), (2,), (0, 1), (1, 2), (0, 2)])
def test_sum(dtype, shape, dims):
    if not is_legal_combine(shape,dims):
        return

    torch.manual_seed(0)
    x = test_common.generate_tensor(shape,dtype).npu()
    grid = (1,1,1)

    y_ref = torch_sum(x,dims)
    y_cal = torch.empty(y_ref.shape, dtype=eval('torch.'+dtype), device="npu")
    if len(dims) == 1: # 1d reduce, 1-3d shape
        append_shape, tt_kernel = shape_map[len(shape)]["append_shape"], shape_map[len(shape)]["func"]
        xnumel, ynumel, znumel = shape + append_shape
        XB, YB, ZB = xnumel, ynumel, znumel
        if (len(shape) == 2) and (x.numel()*x.element_size() > 8192) :
            if dims[0] == 0:
                grid = (1, ynumel, 1)
                YB = 1
            else:
                grid = (xnumel, 1, 1)
                XB = 1
    else: # 3d shape, 2d reduce
        tt_kernel = dims_map[dims]
        xnumel, ynumel, znumel = shape
        XB, YB, ZB = xnumel, ynumel, znumel

    tt_kernel[grid](x, y_cal, xnumel, ynumel, znumel, XB, YB, ZB, dims[0])
    test_common.validate_cmp(dtype, y_cal, y_ref)
